import torch
import torch.nn.functional as F
import numpy as np
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from torch import nn
from torch import optim
import torchvision


##卷积
layer1=nn.Conv2d(1,3,3,1,1)  # 2d的函数卷积运算  输入channel ， kernel_channel（里边是不同的kernel矩阵参数，比如边缘矩阵，模糊矩阵等）
x=torch.randn(1,1,28,28)
out=layer1(x)
print(out.shape)
print(layer1.weight.shape)

#池化采样
x=torch.randn(1,1,28,28)
layer2=nn.MaxPool2d(3,2,1,1,True)
layer3=nn.AvgPool2d(3,2,1,True,True)#return_indices (bool): 如果设置为True，则返回输出最大值的索引，默认为False。
                                                                                #ceil_mode (bool): 如果设置为True，则使用向上取整计算输出形状，默认为False。
out_tuple=layer2(x)
out=out_tuple[0]
print(out.shape)

# 临近采样放大
x=torch.randn(1,1,28,28)
out=F.interpolate(x,(28*2,28*2),mode='bilinear',align_corners=True)
#bilinear:双线性插值常用于图像处理中的缩放、旋转等操作，因为它可以保持图像的平滑性，避免出现锯齿状的边缘。

# Batch-norm
x=torch.rand(1,16,28*28)
layer4=nn.BatchNorm1d(16)
out=layer4(x)
print(layer4.running_mean)
print(layer4.running_var)



########################         nn——Module    ###########
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 定义网络结构


    def forward(self, x):
        # 定义前向传播过程
        pass


#1——现成的layers 模块
'''
Linear  Relu  Sigmoid  Conv2d ConvTranspose2d Dropout Batch Normalization 
'''
#2——Container：一次forward处理多数据——————核心

class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.net=nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.ReLU(True),
            nn.BatchNorm2d(32),

            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.BatchNorm2d(64),

            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.ReLU(True),
            nn.BatchNorm2d(64),

            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.BatchNorm2d(128),

        )

#3——parameters
net=nn.Sequential(nn.Linear(in_features=4, out_features=2),nn.Linear(4,2))
print(list(net.parameters())[0])
print(list(net.named_parameters())[0])
print('*'*50)
print(dict(net.named_parameters()).items())
optimizer=optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

#4——to device :  GPU加速

device=torch.device('cpu')
net=Net()
net.to(device)


#5——save and load： 文件保存，避免断电等问题引起的数据丧失
device = torch.device('cuda')
net=Net()
net.to(device)
net.load_state_dict(torch.load('ckpt.mdl'))
    #train
torch.save(net.state_dict(), 'ckpt.mdl')

#6——train|test 快速状态切换
device = torch.device('cuda')
net=Net()
net.to(device)
#train
net.train()
#test
net.eval()  #使用net.eval()将模型设置为评估模式。在此模式下，上述的训练特性会被禁用，以提高模型的性能和稳定性，特别是在测试或验证阶段。

#7——实现自己的类
class Flatten(nn.Module):
    def __init__(self):
        super(Flatten, self).__init__()
    def forward(self, input):
        return input.view(input.size(0), -1)# -1代表后边纬度合在一起
class TestNet(nn.Module):
    def __init__(self):
        super(TestNet, self).__init__()
        self.net=nn.Sequential(nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),
                               nn.MaxPool2d(kernel_size=2, stride=2),
                               Flatten(),
                               nn.Linear(in_features=1*14*14, out_features=10),
                               )
    def forward(self, x):
        return self.net(x)

class  MyLinear(nn.Module):
    def __init__(self,inp,outp):
        super(MyLinear, self).__init__()
        self.w=nn.Parameter(torch.randn(outp,inp))
        self.b=nn.Parameter(torch.randn(outp,1))
    def forward(self,x):
        x=x@self.w.t()+self.b
        return x
