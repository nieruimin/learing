import torch
import torch.nn.functional as F
import numpy as np
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from torch import nn
from torch import optim
import torchvision
torch.set_default_dtype(torch.float64)

# pytorch 自动求导   pred=xw+b
x=torch.ones(1)
w=torch.full([1],2,requires_grad=True,dtype=torch.float64)
print(x)
print(w)
mse=F.mse_loss(x*w,torch.ones(1))           #  predict=xw   label=1
print(mse)
print(torch.autograd.grad(mse,[w]))   #  loss   [w1,w2~~~]

# softmax
'''
softmax(zi)=   exp(zi)/sum(exp(z))


softmax_grad=   pi*(1-pj)    if     i = j    
                -pi*pj       if     i != j 
'''
a=torch.rand(3,requires_grad=True)
p=F.softmax(a,dim=0)
softmax_grad=torch.autograd.grad(p[1],[a])   #i=j时才是正的
print(softmax_grad)                                 #(tensor([-0.1072,  0.2368, -0.1296]),)

# 函数极小值优化
def func(x):
    return (x[0]**2+x[1]-11)**2 + (x[0]+x[1]**2-7)**2
x=np.arange(-6,6,0.1)
y=np.arange(-6,6,0.1)
X,Y=np.meshgrid(x,y)
Z=func([X,Y])
fig=plt.figure('func')
# ax=fig.gca()
# ax.plot_surface(X,Y,Z)
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z)
ax.view_init(elev=30,azim=-30)      #  调节视角
ax.set_xlabel('x')
ax.set_ylabel('y')
plt.show(block=True)
x=torch.tensor([0.,0.],requires_grad=True)
optimizer=torch.optim.Adam([x],lr=0.001)
# for step in range(30000):
#     pred=func(x)
#     optimizer.zero_grad()
#     pred.backward()
#     optimizer.step()
#     if step%2000==0:
#         print('step{}: x={},f(x)={}'.format(step,x.tolist(),pred.item()))

# cross entropy
'''
cross entropy 适用于分类问题，对于二分类问题，优化目标函数为：H(P|Q)=-(ylog(p)+(1-y)log(1-p)) 与其他不同。我们优化是朝着H(P|Q)
数值减小方向优化，
对于其他分类为题，例如 实际:P=[1,0,0,0,0]  预测:Q=[0.98,0.01,0,0,0.01]  那么H(P|Q)=0.02，符合优化目标
使用cross entropy的原因 :  SIGMOD+MSE  会出现 gradient vanish 问题
'''
x=torch.randn(1,784,requires_grad=True)
w=torch.randn(10,784,requires_grad=True)
logits=x@w.t()
pred=F.softmax(logits,dim=1)        #     1
pred_log=torch.log(pred)            #     2
result1=F.nll_loss(pred_log,torch.tensor([3]))#      3
print(result1)
result2=F.cross_entropy(logits,torch.tensor([3]))  #   等价于 1+2+3  使用更倾向于result2
print(result2)


#  多分类问题实战
batch_size=200
epochs=10
learning_rate=0.01
train_loader=torch.utils.data.DataLoader(
    torchvision.datasets.MNIST('../minist_data', train=True, download=True,
                               transform=torchvision.transforms.Compose([
                                   torchvision.transforms.ToTensor(),
                                   torchvision.transforms.Normalize((0.1307,), (0.3081,))
                               ])),batch_size=batch_size,shuffle=True)
test_loader=torch.utils.data.DataLoader(
    torchvision.datasets.MNIST('../misist_data/', train=False, download=True,
                               transform=torchvision.transforms.Compose([
                                    torchvision.transforms.ToTensor(),
                                    torchvision.transforms.Normalize((0.1307,), (0.3081,))
                                ])),batch_size=batch_size,shuffle=True
)
w1,b1=torch.randn(200,784,requires_grad=True),torch.zeros(200,requires_grad=True)       # 降纬操作  200是out纬度 784 是input纬度
w2,b2=torch.randn(200,200,requires_grad=True),torch.zeros(200,requires_grad=True)
w3,b3=torch.randn(10,200,requires_grad=True),torch.zeros(10,requires_grad=True)
torch.nn.init.kaiming_normal_(w1)
torch.nn.init.kaiming_normal_(w2)           #  一定要进行初始化，否则会出现梯度消失问题
torch.nn.init.kaiming_normal_(w3)
def forward(x):     #三层函数
    x=x@w1.t()+b1
    x=F.relu(x)
    x=x@w2.t()+b2
    x=F.relu(x)
    x=x@w3.t()+b3
    x=F.relu(x)
    return x        #    x 为  logits

optimizer_=optim.SGD([w1,w2,w3,b1,b2,b3],lr=learning_rate)
criteon=nn.CrossEntropyLoss()
for epoch in range(epochs):
    for batch_idx,(data,target) in enumerate(train_loader):
        data=data.view(-1,28*28)
        logits=forward(data)
        loss=criteon(logits,target)
        optimizer_.zero_grad()
        loss.backward()
        optimizer_.step()
        if batch_idx%100==0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),100*batch_idx/len(train_loader),loss.item()))
    test_loss=0
    correct=0
    for data,target in test_loader:
        data=data.view(-1,28*28)
        logits=forward(data)
        test_loss+=criteon(logits,target).item()
        pred=logits.data.max(1)[1]
        correct+=pred.eq(target.data).sum()
    test_loss/=len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{}({:.2f}%)'.format(
        test_loss,correct,len(test_loader.dataset), 100*correct/len(test_loader.dataset)))
